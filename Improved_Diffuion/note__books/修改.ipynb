{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "以下是 **为扩散模型添加标签条件信息的完整方案**，包含关键实现步骤和代码示例：\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 条件嵌入架构设计**\n",
    "```mermaid\n",
    "graph TD\n",
    "    T[时间步t] --> TE[时间嵌入]\n",
    "    Y[标签y] --> LE[标签嵌入]\n",
    "    TE --> FC[融合层]\n",
    "    LE --> FC\n",
    "    FC --> CB[条件向量]\n",
    "    CB --> UNet[UNet各层]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 核心代码修改**\n",
    "\n",
    "#### **(1) 条件嵌入层实现**\n",
    "```python\n",
    "class ConditionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=256, label_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(time_dim, time_dim*4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim*4, time_dim)\n",
    "        )\n",
    "        self.label_embed = nn.Embedding(num_classes, label_dim)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(time_dim + label_dim, time_dim*2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim*2, time_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        # t: [B,] 时间步\n",
    "        # y: [B,] 标签\n",
    "        t_emb = sinusoidal_embedding(t, self.time_embed[0].in_features)\n",
    "        t_emb = self.time_embed(t_emb)  # [B, time_dim]\n",
    "\n",
    "        l_emb = self.label_embed(y)     # [B, label_dim]\n",
    "\n",
    "        # 融合时间与标签信息\n",
    "        combined = torch.cat([t_emb, l_emb], dim=1)\n",
    "        return self.fusion(combined)    # [B, time_dim]\n",
    "```\n",
    "\n",
    "#### **(2) 修改UNet输入**\n",
    "```python\n",
    "class ConditionalUNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.cond_embed = ConditionalEmbedding(num_classes)\n",
    "\n",
    "        # 原UNet结构保持不变，但所有MyBlock需接收条件向量\n",
    "        self.down = nn.ModuleList([\n",
    "            MyBlock(..., cond_dim=256)  # 增加条件输入维度\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, t, y):\n",
    "        cond = self.cond_embed(t, y)\n",
    "\n",
    "        # 将cond传递到每个MyBlock\n",
    "        for block in self.down:\n",
    "            x = block(x, cond)\n",
    "        # ... 后续处理 ...\n",
    "```\n",
    "\n",
    "#### **(3) 调整残差块**\n",
    "```python\n",
    "class MyBlock(nn.Module):\n",
    "    def __init__(self, ..., cond_dim):\n",
    "        super().__init__()\n",
    "        self.cond_proj = nn.Linear(cond_dim, out_ch*2)\n",
    "\n",
    "        # 在卷积后注入条件信息\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(...),\n",
    "            AdaGN(out_ch, cond_dim)  # 自适应组归一化\n",
    "        )\n",
    "\n",
    "class AdaGN(nn.Module):\n",
    "    \"\"\"自适应组归一化\"\"\"\n",
    "    def __init__(self, channels, cond_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(8, channels)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(cond_dim, channels*2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        scale, shift = self.mlp(cond).chunk(2, dim=1)\n",
    "        x = self.norm(x)\n",
    "        return x * (1 + scale[:,:,None,None]) + shift[:,:,None,None]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 训练流程改造**\n",
    "\n",
    "#### **(1) 数据加载器**\n",
    "```python\n",
    "# 假设数据集返回 (data, label)\n",
    "dataset = YourDataset(data_tensor, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "```\n",
    "\n",
    "#### **(2) 训练循环调整**\n",
    "```python\n",
    "for batch in dataloader:\n",
    "    x0, y = batch  # x0: [B,24,50], y: [B]\n",
    "    x0 = x0.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # 加噪过程\n",
    "    t = torch.randint(0, num_steps, (x0.size(0),), device=device)\n",
    "    noise = torch.randn_like(x0)\n",
    "    xt = q_sample(x0, t, noise)\n",
    "\n",
    "    # 条件预测\n",
    "    pred_noise = model(xt, t, y)  # 传入标签y\n",
    "\n",
    "    # 损失计算\n",
    "    loss = F.mse_loss(pred_noise, noise)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 条件采样实现**\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape, labels, device):\n",
    "    \"\"\"带标签条件的采样\"\"\"\n",
    "    img = torch.randn(shape, device=device)\n",
    "    for i in reversed(range(num_steps)):\n",
    "        t = torch.full((shape[0],), i, device=device)\n",
    "        img = p_sample(model, img, t, labels)  # 传入标签\n",
    "    return img\n",
    "\n",
    "def generate_class_samples(model, num_samples, label, device):\n",
    "    labels = torch.full((num_samples,), label, device=device)\n",
    "    samples = p_sample_loop(model, (num_samples,24,50), labels, device)\n",
    "    return samples.cpu().numpy()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. 条件控制策略对比**\n",
    "\n",
    "| 方法 | 优点 | 缺点 | 适用场景 |\n",
    "|------|------|------|---------|\n",
    "| **嵌入拼接** | 实现简单 | 条件信息可能被稀释 | 低维条件 |\n",
    "| **自适应归一化** | 细粒度控制 | 计算量稍大 | 高质量生成 |\n",
    "| **交叉注意力** | 显式对齐 | 需要设计注意力层 | 文本等复杂条件 |\n",
    "\n",
    "---\n",
    "\n",
    "### **6. 效果验证方法**\n",
    "\n",
    "#### **(1) 类别控制测试**\n",
    "```python\n",
    "# 生成不同类别的样本\n",
    "class_labels = [0, 1, 2]\n",
    "for label in class_labels:\n",
    "    samples = generate_class_samples(model, 10, label)\n",
    "    plot_features(samples)  # 可视化特征分布\n",
    "```\n",
    "\n",
    "#### **(2) 条件相似度评估**\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 用分类器评估生成样本的标签一致性\n",
    "classifier = load_pretrained_classifier()\n",
    "gen_labels = classifier.predict(samples)\n",
    "acc = accuracy_score(true_labels, gen_labels)\n",
    "print(f\"条件一致性准确率: {acc:.2%}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **常见问题解决方案**\n",
    "\n",
    "**问题1：条件信息不起作用**\n",
    "- 检查标签嵌入是否参与梯度更新\n",
    "- 验证条件向量的范数是否与时间嵌入相当\n",
    "- 尝试增大标签嵌入维度（如128→256）\n",
    "\n",
    "**问题2：生成样本多样性下降**\n",
    "- 在条件嵌入后添加Dropout（keep_prob=0.9）\n",
    "- 使用标签平滑（Label Smoothing）\n",
    "- 引入随机噪声到条件向量\n",
    "\n",
    "**问题3：训练不稳定**\n",
    "- 对标签嵌入进行L2归一化\n",
    "- 使用梯度裁剪（max_norm=1.0）\n",
    "- 降低条件融合层的学习率\n",
    "\n",
    "---\n",
    "\n",
    "如果需要某个模块的详细实现（如自适应归一化），或想了解如何平衡条件控制与生成多样性，可以告诉我具体方向！"
   ],
   "id": "c7fd45bf6ff195d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9b02493452572bb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "以下是 **将现有U-Net改造为条件U-Net的完整代码修改方案**，包含关键修改点和注释说明：\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 条件U-Net类改造**\n",
    "```python\n",
    "class ConditionalDiffusionUNet(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=128, label_dim=64):\n",
    "        super().__init__()\n",
    "        chs = [1, 64, 128, 256]\n",
    "\n",
    "        # 替换为条件嵌入层\n",
    "        self.cond_embed = ConditionalEmbedding(\n",
    "            num_classes=num_classes,\n",
    "            time_dim=time_dim,\n",
    "            label_dim=label_dim\n",
    "        )\n",
    "        cond_dim = time_dim  # 条件向量的总维度\n",
    "\n",
    "        # 下采样路径（修改所有MyBlock的cond_dim）\n",
    "        self.down = nn.ModuleList([\n",
    "            MySequential(\n",
    "                MyBlock(chs[i], chs[i+1], cond_dim=cond_dim),\n",
    "                MyBlock(chs[i+1], chs[i+1], cond_dim=cond_dim),\n",
    "                Residual(PreNorm(chs[i+1], LinearAttention(chs[i+1])))\n",
    "            ) for i in range(len(chs)-1)\n",
    "        ])\n",
    "\n",
    "        # 中间层\n",
    "        self.mid = MySequential(\n",
    "            MyBlock(chs[-1], chs[-1], cond_dim=cond_dim),\n",
    "            Residual(PreNorm(chs[-1], Attention(chs[-1]))),\n",
    "            MyBlock(chs[-1], chs[-1], cond_dim=cond_dim)\n",
    "        )\n",
    "\n",
    "        # 上采样路径\n",
    "        self.up = nn.ModuleList([\n",
    "            MySequential(\n",
    "                MyBlock(chs[i+1]*2, chs[i], cond_dim=cond_dim),\n",
    "                MyBlock(chs[i], chs[i], cond_dim=cond_dim),\n",
    "                Residual(PreNorm(chs[i], LinearAttention(chs[i])))\n",
    "            ) for i in reversed(range(len(chs)-1))\n",
    "        ])\n",
    "\n",
    "        self.final = nn.Conv2d(chs[0], 1, 1)\n",
    "\n",
    "    def forward(self, x, t, y):\n",
    "        \"\"\"新增标签y作为输入\"\"\"\n",
    "        x = x.unsqueeze(1)  # [B,1,24,50]\n",
    "        cond_emb = self.cond_embed(t, y)  # 获取融合条件向量\n",
    "        skips = []\n",
    "\n",
    "        # 编码器（传递cond_emb）\n",
    "        for block in self.down:\n",
    "            x = block(x, cond_emb)\n",
    "            skips.append(x)\n",
    "            x = F.max_pool2d(x, kernel_size=(2,1))\n",
    "\n",
    "        # 中间处理\n",
    "        x = self.mid(x, cond_emb)\n",
    "\n",
    "        # 解码器\n",
    "        for i, block in enumerate(self.up):\n",
    "            x = F.interpolate(x, scale_factor=(2,1), mode='nearest')\n",
    "            x = torch.cat([x, skips[-(i+1)]], dim=1)\n",
    "            x = block(x, cond_emb)\n",
    "\n",
    "        return self.final(x).squeeze(1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 修改MyBlock支持条件输入**\n",
    "```python\n",
    "class MyBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, cond_dim):\n",
    "        super().__init__()\n",
    "        # 条件投影层\n",
    "        self.cond_proj = nn.Sequential(\n",
    "            nn.Linear(cond_dim, out_ch * 2),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.ds_conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        \"\"\"新增cond参数\"\"\"\n",
    "        h = self.ds_conv(x)\n",
    "\n",
    "        # 条件注入\n",
    "        scale, shift = self.cond_proj(cond).chunk(2, dim=1)\n",
    "        h = h * (1 + scale[:, :, None, None]) + shift[:, :, None, None]\n",
    "\n",
    "        h = self.conv(h)\n",
    "        return h + self.res_conv(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 配套修改训练流程**\n",
    "```python\n",
    "# 训练循环修改\n",
    "for batch in dataloader:\n",
    "    x0, labels = batch  # 假设数据返回 (数据, 标签)\n",
    "    x0 = x0.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # 加噪过程\n",
    "    t = torch.randint(0, num_steps, (x0.size(0),), device=device)\n",
    "    noise = torch.randn_like(x0)\n",
    "    xt = q_sample(x0, t, noise)\n",
    "\n",
    "    # 条件预测\n",
    "    pred_noise = model(xt, t, labels)  # 传入标签\n",
    "\n",
    "    # 计算损失\n",
    "    loss = F.mse_loss(pred_noise, noise)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 条件采样函数修改**\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape, labels, device):\n",
    "    \"\"\"带标签条件的采样\"\"\"\n",
    "    img = torch.randn(shape, device=device)\n",
    "    for i in reversed(range(num_steps)):\n",
    "        t = torch.full((shape[0],), i, device=device, dtype=torch.long)\n",
    "        img = p_sample(model, img, t, labels)\n",
    "    return img\n",
    "\n",
    "def generate_class_samples(model, num_samples, label, device):\n",
    "    labels = torch.full((num_samples,), label, device=device)\n",
    "    samples = p_sample_loop(model, (num_samples,24,50), labels, device)\n",
    "    return samples.cpu().numpy()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **关键修改点说明**\n",
    "| 模块 | 修改内容 | 作用 |\n",
    "|------|----------|------|\n",
    "| **U-Net初始化** | 添加`num_classes`参数 | 支持标签输入 |\n",
    "| **条件嵌入** | 替换`time_embed`为`cond_embed` | 融合时间+标签信息 |\n",
    "| **MyBlock** | 新增`cond_proj`和条件缩放 | 实现条件特征调制 |\n",
    "| **前向传播** | 所有模块传递`cond_emb` | 条件信息贯穿网络 |\n",
    "\n",
    "---\n",
    "\n",
    "### **验证测试**\n",
    "```python\n",
    "def test_conditional_unet():\n",
    "    num_classes = 10\n",
    "    model = ConditionalDiffusionUNet(num_classes=num_classes).cuda()\n",
    "\n",
    "    # 测试输入\n",
    "    x = torch.randn(4, 24, 50).cuda()\n",
    "    t = torch.randint(0, 1000, (4,)).cuda()\n",
    "    labels = torch.randint(0, num_classes, (4,)).cuda()\n",
    "\n",
    "    # 前向传播\n",
    "    output = model(x, t, labels)\n",
    "    assert output.shape == (4, 24, 50), \"输出形状错误\"\n",
    "\n",
    "    # 梯度测试\n",
    "    loss = output.mean()\n",
    "    loss.backward()\n",
    "    print(\"梯度测试通过\")\n",
    "\n",
    "test_conditional_unet()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **可能遇到的问题及解决**\n",
    "1. **维度不匹配错误**\n",
    "   - 检查`cond_embed`的输出维度是否与`MyBlock`的`cond_dim`一致\n",
    "   - 确保所有`cond_proj`层的输入维度匹配\n",
    "\n",
    "2. **标签泄漏问题**\n",
    "   - 验证在采样时是否使用正确的标签输入\n",
    "   - 检查训练时标签是否与数据正确对应\n",
    "\n",
    "3. **条件控制不足**\n",
    "   - 增大标签嵌入维度（如128→256）\n",
    "   - 在条件投影后添加非线性激活\n",
    "\n",
    "需要我解释条件注入的具体实现细节吗？或是展示如何可视化条件控制效果？"
   ],
   "id": "45f8f2134d4c97c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
