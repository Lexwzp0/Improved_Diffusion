{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.510390Z",
     "start_time": "2025-03-17T12:23:06.505390Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from data.pyg_dataToGraph import DataToGraph\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.744093Z",
     "start_time": "2025-03-17T12:23:06.546515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO 加载数据集\n",
    "dataset = DataToGraph(\n",
    "    raw_data_path='../data/raw',\n",
    "    dataset_name='TFF' + '.mat')  # 格式: [(graph,label),...,(graph,label)]\n",
    "\n",
    "input_dim = dataset[0].x.size(1)\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# 提取所有的x和y\n",
    "x0 = []\n",
    "labels = []\n",
    "\n",
    "for data in dataset:\n",
    "    # 提取x (形状为 [num_nodes, input_dim])\n",
    "    # 但是你提到dataset.x的形状是 [24,50]，这可能是一个图的x特征矩阵\n",
    "    x0.append(data.x)\n",
    "    # 提取y（标量标签）\n",
    "    labels.append(data.y)\n",
    "\n",
    "# 将列表转换为张量\n",
    "x0 = torch.stack(x0)  # 形状 [num_samples, 24, 50]\n",
    "labels = torch.stack(labels)  # 形状 [num_samples]\n",
    "\n",
    "print(num_classes)\n",
    "print(\"X0 shape:\", x0.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ],
   "id": "f6e1a3fc4eb53ea2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X0 shape: torch.Size([2368, 24, 50])\n",
      "Labels shape: torch.Size([2368, 1])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.765247Z",
     "start_time": "2025-03-17T12:23:06.759107Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
   "id": "6a52cbd962af9c9a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.871374Z",
     "start_time": "2025-03-17T12:23:06.854644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将数据传输到GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO 确定超参数的值\n",
    "# 超参数值\n",
    "num_steps = 1000  # 假设扩散步数为 1000\n",
    "eps = 1e-5  # 避免除以零或引入过小的数值的小偏移量\n",
    "\n",
    "# 生成时间步的序列\n",
    "t = torch.linspace(0, 1, num_steps + 1)  # 主要时间步范围从 0 到 1\n",
    "\n",
    "# 使用余弦调度生成 betas\n",
    "betas = torch.cos(torch.pi / 2.0 * t) ** 2  # 余弦平方函数\n",
    "betas = betas / betas.max()  # 归一化到 0-1 范围\n",
    "betas = torch.flip(betas, [0])  # 反转顺序，以确保从小到大递增\n",
    "betas = torch.clamp(betas, min=1e-5, max=0.5e-2)  # 调整范围到 (1e-5, 0.5e-2)\n",
    "\n",
    "# 计算 alpha , alpha_prod , alpha_prod_previous , alpha_bar_sqrt 等变量的值\n",
    "alphas = 1 - betas\n",
    "alphas_prod = torch.cumprod(alphas, dim=0)  # 累积连乘\n",
    "alphas_prod_p = torch.cat([torch.tensor([1]).float(), alphas_prod[:-1]], 0)  # p means previous\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod)\n",
    "one_minus_alphas_bar_log = torch.log(1 - alphas_prod)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)\n",
    "\n",
    "# 将超参数也移动到GPU\n",
    "betas = betas.to(device)\n",
    "alphas = alphas.to(device)\n",
    "alphas_prod = alphas_prod.to(device)\n",
    "alphas_prod_p = alphas_prod_p.to(device)\n",
    "alphas_bar_sqrt = alphas_bar_sqrt.to(device)\n",
    "one_minus_alphas_bar_log = one_minus_alphas_bar_log.to(device)\n",
    "one_minus_alphas_bar_sqrt = one_minus_alphas_bar_sqrt.to(device)\n",
    "\n",
    "assert alphas_prod.shape == alphas_prod.shape == alphas_prod_p.shape \\\n",
    "       == alphas_bar_sqrt.shape == one_minus_alphas_bar_log.shape \\\n",
    "       == one_minus_alphas_bar_sqrt.shape\n",
    "print(\"all the same shape:\", betas.shape)"
   ],
   "id": "1a7ea83357d98794",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the same shape: torch.Size([1001])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.912632Z",
     "start_time": "2025-03-17T12:23:06.907629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def q_sample(x0, t, noise):\n",
    "    \"\"\"前向扩散过程：根据时间步t给x0加噪\"\"\"\n",
    "    sqrt_alpha_prod = torch.sqrt(alphas_prod[t]).view(-1, 1, 1)\n",
    "    sqrt_one_minus_alpha_prod = torch.sqrt(1 - alphas_prod[t]).view(-1, 1, 1)\n",
    "    return sqrt_alpha_prod * x0 + sqrt_one_minus_alpha_prod * noise"
   ],
   "id": "2339b8d5d325cdc4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MySequential",
   "id": "eb8c19daf22706b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.945575Z",
     "start_time": "2025-03-17T12:23:06.940427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MySequential(nn.Sequential):\n",
    "    def forward(self, x, t_emb):\n",
    "        for module in self:\n",
    "            if isinstance(module, ConditionalBlock):  # 仅对特定模块传参\n",
    "                x = module(x, t_emb)\n",
    "            else:  # 其他模块按默认方式处理\n",
    "                x = module(x)\n",
    "        return x"
   ],
   "id": "6afdb1786819d31c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conditional Embedding",
   "id": "7cfc68705c4612c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.957810Z",
     "start_time": "2025-03-17T12:23:06.950582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "def sinusoidal_embedding(t, dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t: 时间步张量 [batch_size, ]\n",
    "        dim: 嵌入维度\n",
    "    Returns:\n",
    "        嵌入向量 [batch_size, dim]\n",
    "    \"\"\"\n",
    "    device = t.device\n",
    "    half_dim = dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "    emb = t.float()[:, None] * emb[None, :]  # [batch_size, half_dim]\n",
    "\n",
    "    # 拼接正弦和余弦分量\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "\n",
    "    # 处理奇数维度情况\n",
    "    if dim % 2 == 1:\n",
    "        emb = F.pad(emb, (0, 1), mode='constant')\n",
    "\n",
    "    return emb\n",
    "\n",
    "\n",
    "class ConditionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=256, label_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim)\n",
    "        )\n",
    "        self.label_embed = nn.Embedding(num_classes, label_dim)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(time_dim + label_dim, time_dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 2, time_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        # t: [B,] 时间步\n",
    "        # y: [B,] 标签\n",
    "        t_emb = sinusoidal_embedding(t, self.time_embed[0].in_features)\n",
    "        t_emb = self.time_embed(t_emb)  # [B, time_dim]\n",
    "\n",
    "        l_emb = self.label_embed(y).squeeze(1)  # [B, label_dim]\n",
    "\n",
    "        # 融合时间与标签信息\n",
    "        combined = torch.cat([t_emb, l_emb], dim=1)\n",
    "        return self.fusion(combined)  # [B, time_dim]"
   ],
   "id": "ef33d22a52c59b27",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.976993Z",
     "start_time": "2025-03-17T12:23:06.971086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EnhancedTimeEmbedding(nn.Module):\n",
    "    \"\"\"增强时间嵌入（添加多层感知）\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.GELU(),\n",
    "            #nn.SiLU(),\n",
    "            nn.Linear(dim*4, dim),\n",
    "            #nn.SiLU(),\n",
    "            #nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return self.embed(emb)"
   ],
   "id": "46439c1ffa7511a9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conditional Block",
   "id": "f0196aceaeb0ed22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.006042Z",
     "start_time": "2025-03-17T12:23:06.991613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from einops import rearrange\n",
    "\n",
    "class ConditionalBlock(nn.Module):\n",
    "    \"\"\"基于你原有MyBlock改造的条件版本\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, cond_dim, mult=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cond_dim: 条件向量的维度 (time+label的融合维度)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 修改后的条件投影层（移除偏置项）验证条件注入的有效性\n",
    "        self.cond_mlp = nn.Sequential(\n",
    "            nn.Linear(cond_dim, out_ch*2, bias=False),  # 关键修改：bias=False\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        # 保持原有卷积结构\n",
    "        self.ds_conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.GroupNorm(1, out_ch),\n",
    "            nn.Conv2d(out_ch, out_ch * mult, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.GroupNorm(1, out_ch * mult),\n",
    "            nn.Conv2d(out_ch * mult, out_ch, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond_emb):\n",
    "        \"\"\"输入变化：t_emb → cond_emb (融合时间+标签的条件向量)\"\"\"\n",
    "        h = self.ds_conv(x)\n",
    "\n",
    "        # 条件注入 (scale and shift)\n",
    "        scale, shift = self.cond_mlp(cond_emb).chunk(2, dim=1)  # [B, 2*out_ch] → [B, out_ch], [B, out_ch]\n",
    "        h = h * (1 + scale[:, :, None, None])  # 缩放\n",
    "        h = h + shift[:, :, None, None]        # 偏移\n",
    "\n",
    "        h = self.conv(h)\n",
    "        return h + self.res_conv(x)  # 保持原有残差连接"
   ],
   "id": "13490e2ab9eb36ea",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Residual",
   "id": "9375f275ff35bf03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.026991Z",
     "start_time": "2025-03-17T12:23:07.021019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 残差模块，将输入加到输出上\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x"
   ],
   "id": "edd6143460bc48f3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention",
   "id": "27a8e4973ba08f6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.073523Z",
     "start_time": "2025-03-17T12:23:07.065393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Attention\n",
    "from torch import einsum, softmax\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b\"\n",
    "                     \" h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)"
   ],
   "id": "ae17307e2f1ce159",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PreNorm",
   "id": "7a2a7cda29003ea7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.109099Z",
     "start_time": "2025-03-17T12:23:07.102642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ],
   "id": "442d84136cf723b2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### NoisyDataClassifier",
   "id": "9c58bde2bdf3ebe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.132105Z",
     "start_time": "2025-03-17T12:23:07.124106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "class UNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=128):\n",
    "        super().__init__()\n",
    "        chs = [1, 64, 128, 256]  # 输入通道调整为1 (单通道特征图)\n",
    "\n",
    "        # 时间嵌入层 (移除标签条件)\n",
    "        self.time_embed = EnhancedTimeEmbedding(time_dim)\n",
    "\n",
    "        # 下采样路径 (增强特征提取)\n",
    "        self.down = nn.ModuleList([\n",
    "            MySequential(\n",
    "                ConditionalBlock(chs[i], chs[i+1], cond_dim=time_dim),\n",
    "                ConditionalBlock(chs[i+1], chs[i+1], cond_dim=time_dim),\n",
    "                Residual(PreNorm(chs[i+1], LinearAttention(chs[i+1])))\n",
    "            ) for i in range(len(chs)-1)\n",
    "        ])\n",
    "\n",
    "        # 中间层 (适配分类任务)\n",
    "        self.mid = MySequential(\n",
    "            ConditionalBlock(chs[-1], chs[-1]*2, cond_dim=time_dim),\n",
    "            Residual(PreNorm(chs[-1]*2, Attention(chs[-1]*2))),\n",
    "            ConditionalBlock(chs[-1]*2, chs[-1], cond_dim=time_dim)\n",
    "        )\n",
    "\n",
    "        # 分类头\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),  # 全局平均池化\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(chs[-1], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        输入:\n",
    "            x: [B, H, W] (如 [B,24,50])\n",
    "            t: [B,] 时间步\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)  # [B,1,24,50]\n",
    "\n",
    "        # 时间嵌入\n",
    "        t_emb = self.time_embed(t)\n",
    "\n",
    "        skips = []\n",
    "\n",
    "        # 编码器\n",
    "        for block in self.down:\n",
    "            x = block(x, t_emb)\n",
    "            skips.append(x)\n",
    "            x = F.max_pool2d(x, kernel_size=(2,1))  # 保持宽度不变\n",
    "\n",
    "        # 中间处理\n",
    "        x = self.mid(x, t_emb)\n",
    "\n",
    "        # 分类\n",
    "        return self.classifier(x)\n"
   ],
   "id": "6867136b77fada9c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###",
   "id": "1ce081f9fcfc91d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### train_Classifier",
   "id": "325ee603e8195520"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.156410Z",
     "start_time": "2025-03-17T12:23:07.149401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设相关自定义模块已定义（ConditionalBlock, LinearAttention等）\n",
    "# 若实际模块定义不同，需调整此处导入\n",
    "\n",
    "def test_unet_classifier():\n",
    "    # 配置参数\n",
    "    batch_size = 4\n",
    "    input_shape = (24, 50)  # 高 x 宽\n",
    "    num_classes = 7\n",
    "    time_dim = 128\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 固定随机种子\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # 初始化模型\n",
    "    model = UNetClassifier(num_classes=num_classes, time_dim=time_dim).to(device)\n",
    "\n",
    "    # 生成测试数据\n",
    "    x_test = torch.randn(batch_size, *input_shape).to(device)  # [4,24,50]\n",
    "    t_test = torch.randint(0, 1000, (batch_size,)).to(device)  # 随机时间步 [4,]\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_test, t_test)\n",
    "\n",
    "    # 验证输出维度\n",
    "    expected_shape = (batch_size, num_classes)\n",
    "    assert logits.shape == expected_shape, (\n",
    "        f\"输出形状错误！期望: {expected_shape}, 实际: {logits.shape}\"\n",
    "    )\n",
    "\n",
    "    # 打印测试结果\n",
    "    print(f\"输入形状: {x_test.shape}\")\n",
    "    print(f\"时间步形状: {t_test.shape}\")\n",
    "    print(f\"输出logits形状: {logits.shape}\")\n",
    "    print(\"测试通过！\")"
   ],
   "id": "eb70a7d0e3c2ce9e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.557094Z",
     "start_time": "2025-03-17T12:23:07.171609Z"
    }
   },
   "cell_type": "code",
   "source": "test_unet_classifier()",
   "id": "94eba1bc1df4e44d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([4, 24, 50])\n",
      "时间步形状: torch.Size([4])\n",
      "输出logits形状: torch.Size([4, 7])\n",
      "测试通过！\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6cc86a49156676fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:18.114909Z",
     "start_time": "2025-03-17T12:23:07.571095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_conditional_classifier(\n",
    "    x0,                  # 原始数据 [N,24,50]\n",
    "    labels,              # 标签 [N]\n",
    "    num_classes,         # 类别数\n",
    "    num_steps=1000,      # 扩散总步数\n",
    "    batch_size=128,\n",
    "    lr=1e-4,\n",
    "    epochs=500,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_path='best_classifier.pth'\n",
    "):\n",
    "    # 数据预处理\n",
    "    dataset = TensorDataset(x0, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    # 初始化模型\n",
    "    model = UNetClassifier(num_classes=num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    # 训练记录\n",
    "    best_loss = float('inf')\n",
    "    history = {'train_loss': [], 'acc': []}\n",
    "\n",
    "    print(f\"\\n🚀 开始训练 | 设备: {device}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"样本数: {len(x0)} | 类别数: {num_classes}\")\n",
    "    print(f\"批次大小: {batch_size} | 初始学习率: {lr}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for batch_x0, batch_y in pbar:\n",
    "            # 数据转移至设备\n",
    "            batch_x0 = batch_x0.to(device)        # [B,24,50]\n",
    "            batch_y = batch_y.to(device)          # [B]\n",
    "\n",
    "            # 生成随机时间步\n",
    "            b = batch_x0.size(0)\n",
    "            t = torch.randint(0, num_steps, (b,), device=device)\n",
    "\n",
    "            # 前向加噪\n",
    "            noise = torch.randn_like(batch_x0)\n",
    "            sqrt_alpha = torch.sqrt(alphas_prod[t]).view(-1,1,1)\n",
    "            sqrt_one_minus_alpha = torch.sqrt(1 - alphas_prod[t]).view(-1,1,1)\n",
    "            noisy_x = sqrt_alpha * batch_x0 + sqrt_one_minus_alpha * noise\n",
    "\n",
    "            # 模型前向\n",
    "            logits = model(noisy_x, t)\n",
    "\n",
    "            batch_y = batch_y.squeeze(1)\n",
    "            # 计算损失\n",
    "            loss = F.cross_entropy(logits, batch_y)\n",
    "\n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计指标\n",
    "            epoch_loss += loss.item() * b\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total += b\n",
    "\n",
    "            # 更新进度条\n",
    "            pbar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'acc': f\"{correct/total:.2%}\"\n",
    "            })\n",
    "\n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "        # 计算epoch指标\n",
    "        epoch_loss /= len(dataset)\n",
    "        epoch_acc = correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"💾 保存最佳模型 | 损失: {best_loss:.4f}\")\n",
    "\n",
    "        # 打印epoch结果\n",
    "        print(f\"Epoch {epoch:03d} | \"\n",
    "              f\"Loss: {epoch_loss:.4f} | \"\n",
    "              f\"Acc: {epoch_acc:.2%} | \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    print(\"✅ 训练完成!\")\n",
    "    return model, history\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设已加载数据\n",
    "    # x0: [N,24,50], labels: [N]\n",
    "\n",
    "    model, history = train_conditional_classifier(\n",
    "        x0=x0,\n",
    "        labels=labels,\n",
    "        num_classes=num_classes,\n",
    "        num_steps=200,\n",
    "        batch_size=64,\n",
    "        epochs=200,\n",
    "        save_path='best_noisy_classifier.pth'\n",
    "    )\n"
   ],
   "id": "25ab5158e53825ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 开始训练 | 设备: cpu\n",
      "---------------------------------------\n",
      "样本数: 2368 | 类别数: 7\n",
      "批次大小: 64 | 初始学习率: 0.0001\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   5%|▌         | 2/37 [00:10<02:59,  5.14s/it, loss=2.02, acc=17.97%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 111\u001B[39m\n\u001B[32m    106\u001B[39m \u001B[38;5;66;03m# 使用示例\u001B[39;00m\n\u001B[32m    107\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    108\u001B[39m     \u001B[38;5;66;03m# 假设已加载数据\u001B[39;00m\n\u001B[32m    109\u001B[39m     \u001B[38;5;66;03m# x0: [N,24,50], labels: [N]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m111\u001B[39m     model, history = \u001B[43mtrain_conditional_classifier\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    112\u001B[39m \u001B[43m        \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m=\u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    113\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    114\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    116\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    117\u001B[39m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[43m        \u001B[49m\u001B[43msave_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbest_noisy_classifier.pth\u001B[39;49m\u001B[33;43m'\u001B[39;49m\n\u001B[32m    119\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 66\u001B[39m, in \u001B[36mtrain_conditional_classifier\u001B[39m\u001B[34m(x0, labels, num_classes, num_steps, batch_size, lr, epochs, device, save_path)\u001B[39m\n\u001B[32m     64\u001B[39m \u001B[38;5;66;03m# 反向传播\u001B[39;00m\n\u001B[32m     65\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m66\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     67\u001B[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001B[32m1.0\u001B[39m)\n\u001B[32m     68\u001B[39m optimizer.step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    617\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    618\u001B[39m         Tensor.backward,\n\u001B[32m    619\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    624\u001B[39m         inputs=inputs,\n\u001B[32m    625\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m626\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    628\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    821\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    822\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m823\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    824\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    825\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    826\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    827\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
