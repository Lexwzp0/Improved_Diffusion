{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.510390Z",
     "start_time": "2025-03-17T12:23:06.505390Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from data.pyg_dataToGraph import DataToGraph\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.744093Z",
     "start_time": "2025-03-17T12:23:06.546515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO åŠ è½½æ•°æ®é›†\n",
    "dataset = DataToGraph(\n",
    "    raw_data_path='../data/raw',\n",
    "    dataset_name='TFF' + '.mat')  # æ ¼å¼: [(graph,label),...,(graph,label)]\n",
    "\n",
    "input_dim = dataset[0].x.size(1)\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# æå–æ‰€æœ‰çš„xå’Œy\n",
    "x0 = []\n",
    "labels = []\n",
    "\n",
    "for data in dataset:\n",
    "    # æå–x (å½¢çŠ¶ä¸º [num_nodes, input_dim])\n",
    "    # ä½†æ˜¯ä½ æåˆ°dataset.xçš„å½¢çŠ¶æ˜¯ [24,50]ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªå›¾çš„xç‰¹å¾çŸ©é˜µ\n",
    "    x0.append(data.x)\n",
    "    # æå–yï¼ˆæ ‡é‡æ ‡ç­¾ï¼‰\n",
    "    labels.append(data.y)\n",
    "\n",
    "# å°†åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡\n",
    "x0 = torch.stack(x0)  # å½¢çŠ¶ [num_samples, 24, 50]\n",
    "labels = torch.stack(labels)  # å½¢çŠ¶ [num_samples]\n",
    "\n",
    "print(num_classes)\n",
    "print(\"X0 shape:\", x0.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ],
   "id": "f6e1a3fc4eb53ea2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X0 shape: torch.Size([2368, 24, 50])\n",
      "Labels shape: torch.Size([2368, 1])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.765247Z",
     "start_time": "2025-03-17T12:23:06.759107Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
   "id": "6a52cbd962af9c9a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.871374Z",
     "start_time": "2025-03-17T12:23:06.854644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# å°†æ•°æ®ä¼ è¾“åˆ°GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO ç¡®å®šè¶…å‚æ•°çš„å€¼\n",
    "# è¶…å‚æ•°å€¼\n",
    "num_steps = 1000  # å‡è®¾æ‰©æ•£æ­¥æ•°ä¸º 1000\n",
    "eps = 1e-5  # é¿å…é™¤ä»¥é›¶æˆ–å¼•å…¥è¿‡å°çš„æ•°å€¼çš„å°åç§»é‡\n",
    "\n",
    "# ç”Ÿæˆæ—¶é—´æ­¥çš„åºåˆ—\n",
    "t = torch.linspace(0, 1, num_steps + 1)  # ä¸»è¦æ—¶é—´æ­¥èŒƒå›´ä» 0 åˆ° 1\n",
    "\n",
    "# ä½¿ç”¨ä½™å¼¦è°ƒåº¦ç”Ÿæˆ betas\n",
    "betas = torch.cos(torch.pi / 2.0 * t) ** 2  # ä½™å¼¦å¹³æ–¹å‡½æ•°\n",
    "betas = betas / betas.max()  # å½’ä¸€åŒ–åˆ° 0-1 èŒƒå›´\n",
    "betas = torch.flip(betas, [0])  # åè½¬é¡ºåºï¼Œä»¥ç¡®ä¿ä»å°åˆ°å¤§é€’å¢\n",
    "betas = torch.clamp(betas, min=1e-5, max=0.5e-2)  # è°ƒæ•´èŒƒå›´åˆ° (1e-5, 0.5e-2)\n",
    "\n",
    "# è®¡ç®— alpha , alpha_prod , alpha_prod_previous , alpha_bar_sqrt ç­‰å˜é‡çš„å€¼\n",
    "alphas = 1 - betas\n",
    "alphas_prod = torch.cumprod(alphas, dim=0)  # ç´¯ç§¯è¿ä¹˜\n",
    "alphas_prod_p = torch.cat([torch.tensor([1]).float(), alphas_prod[:-1]], 0)  # p means previous\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod)\n",
    "one_minus_alphas_bar_log = torch.log(1 - alphas_prod)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)\n",
    "\n",
    "# å°†è¶…å‚æ•°ä¹Ÿç§»åŠ¨åˆ°GPU\n",
    "betas = betas.to(device)\n",
    "alphas = alphas.to(device)\n",
    "alphas_prod = alphas_prod.to(device)\n",
    "alphas_prod_p = alphas_prod_p.to(device)\n",
    "alphas_bar_sqrt = alphas_bar_sqrt.to(device)\n",
    "one_minus_alphas_bar_log = one_minus_alphas_bar_log.to(device)\n",
    "one_minus_alphas_bar_sqrt = one_minus_alphas_bar_sqrt.to(device)\n",
    "\n",
    "assert alphas_prod.shape == alphas_prod.shape == alphas_prod_p.shape \\\n",
    "       == alphas_bar_sqrt.shape == one_minus_alphas_bar_log.shape \\\n",
    "       == one_minus_alphas_bar_sqrt.shape\n",
    "print(\"all the same shape:\", betas.shape)"
   ],
   "id": "1a7ea83357d98794",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the same shape: torch.Size([1001])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.912632Z",
     "start_time": "2025-03-17T12:23:06.907629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def q_sample(x0, t, noise):\n",
    "    \"\"\"å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼šæ ¹æ®æ—¶é—´æ­¥tç»™x0åŠ å™ª\"\"\"\n",
    "    sqrt_alpha_prod = torch.sqrt(alphas_prod[t]).view(-1, 1, 1)\n",
    "    sqrt_one_minus_alpha_prod = torch.sqrt(1 - alphas_prod[t]).view(-1, 1, 1)\n",
    "    return sqrt_alpha_prod * x0 + sqrt_one_minus_alpha_prod * noise"
   ],
   "id": "2339b8d5d325cdc4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MySequential",
   "id": "eb8c19daf22706b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.945575Z",
     "start_time": "2025-03-17T12:23:06.940427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MySequential(nn.Sequential):\n",
    "    def forward(self, x, t_emb):\n",
    "        for module in self:\n",
    "            if isinstance(module, ConditionalBlock):  # ä»…å¯¹ç‰¹å®šæ¨¡å—ä¼ å‚\n",
    "                x = module(x, t_emb)\n",
    "            else:  # å…¶ä»–æ¨¡å—æŒ‰é»˜è®¤æ–¹å¼å¤„ç†\n",
    "                x = module(x)\n",
    "        return x"
   ],
   "id": "6afdb1786819d31c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conditional Embedding",
   "id": "7cfc68705c4612c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.957810Z",
     "start_time": "2025-03-17T12:23:06.950582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "def sinusoidal_embedding(t, dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t: æ—¶é—´æ­¥å¼ é‡ [batch_size, ]\n",
    "        dim: åµŒå…¥ç»´åº¦\n",
    "    Returns:\n",
    "        åµŒå…¥å‘é‡ [batch_size, dim]\n",
    "    \"\"\"\n",
    "    device = t.device\n",
    "    half_dim = dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "    emb = t.float()[:, None] * emb[None, :]  # [batch_size, half_dim]\n",
    "\n",
    "    # æ‹¼æ¥æ­£å¼¦å’Œä½™å¼¦åˆ†é‡\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "\n",
    "    # å¤„ç†å¥‡æ•°ç»´åº¦æƒ…å†µ\n",
    "    if dim % 2 == 1:\n",
    "        emb = F.pad(emb, (0, 1), mode='constant')\n",
    "\n",
    "    return emb\n",
    "\n",
    "\n",
    "class ConditionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=256, label_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim)\n",
    "        )\n",
    "        self.label_embed = nn.Embedding(num_classes, label_dim)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(time_dim + label_dim, time_dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 2, time_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        # t: [B,] æ—¶é—´æ­¥\n",
    "        # y: [B,] æ ‡ç­¾\n",
    "        t_emb = sinusoidal_embedding(t, self.time_embed[0].in_features)\n",
    "        t_emb = self.time_embed(t_emb)  # [B, time_dim]\n",
    "\n",
    "        l_emb = self.label_embed(y).squeeze(1)  # [B, label_dim]\n",
    "\n",
    "        # èåˆæ—¶é—´ä¸æ ‡ç­¾ä¿¡æ¯\n",
    "        combined = torch.cat([t_emb, l_emb], dim=1)\n",
    "        return self.fusion(combined)  # [B, time_dim]"
   ],
   "id": "ef33d22a52c59b27",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:06.976993Z",
     "start_time": "2025-03-17T12:23:06.971086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EnhancedTimeEmbedding(nn.Module):\n",
    "    \"\"\"å¢å¼ºæ—¶é—´åµŒå…¥ï¼ˆæ·»åŠ å¤šå±‚æ„ŸçŸ¥ï¼‰\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.GELU(),\n",
    "            #nn.SiLU(),\n",
    "            nn.Linear(dim*4, dim),\n",
    "            #nn.SiLU(),\n",
    "            #nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return self.embed(emb)"
   ],
   "id": "46439c1ffa7511a9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conditional Block",
   "id": "f0196aceaeb0ed22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.006042Z",
     "start_time": "2025-03-17T12:23:06.991613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from einops import rearrange\n",
    "\n",
    "class ConditionalBlock(nn.Module):\n",
    "    \"\"\"åŸºäºä½ åŸæœ‰MyBlockæ”¹é€ çš„æ¡ä»¶ç‰ˆæœ¬\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, cond_dim, mult=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cond_dim: æ¡ä»¶å‘é‡çš„ç»´åº¦ (time+labelçš„èåˆç»´åº¦)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # ä¿®æ”¹åçš„æ¡ä»¶æŠ•å½±å±‚ï¼ˆç§»é™¤åç½®é¡¹ï¼‰éªŒè¯æ¡ä»¶æ³¨å…¥çš„æœ‰æ•ˆæ€§\n",
    "        self.cond_mlp = nn.Sequential(\n",
    "            nn.Linear(cond_dim, out_ch*2, bias=False),  # å…³é”®ä¿®æ”¹ï¼šbias=False\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        # ä¿æŒåŸæœ‰å·ç§¯ç»“æ„\n",
    "        self.ds_conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.GroupNorm(1, out_ch),\n",
    "            nn.Conv2d(out_ch, out_ch * mult, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.GroupNorm(1, out_ch * mult),\n",
    "            nn.Conv2d(out_ch * mult, out_ch, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond_emb):\n",
    "        \"\"\"è¾“å…¥å˜åŒ–ï¼št_emb â†’ cond_emb (èåˆæ—¶é—´+æ ‡ç­¾çš„æ¡ä»¶å‘é‡)\"\"\"\n",
    "        h = self.ds_conv(x)\n",
    "\n",
    "        # æ¡ä»¶æ³¨å…¥ (scale and shift)\n",
    "        scale, shift = self.cond_mlp(cond_emb).chunk(2, dim=1)  # [B, 2*out_ch] â†’ [B, out_ch], [B, out_ch]\n",
    "        h = h * (1 + scale[:, :, None, None])  # ç¼©æ”¾\n",
    "        h = h + shift[:, :, None, None]        # åç§»\n",
    "\n",
    "        h = self.conv(h)\n",
    "        return h + self.res_conv(x)  # ä¿æŒåŸæœ‰æ®‹å·®è¿æ¥"
   ],
   "id": "13490e2ab9eb36ea",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Residual",
   "id": "9375f275ff35bf03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.026991Z",
     "start_time": "2025-03-17T12:23:07.021019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# æ®‹å·®æ¨¡å—ï¼Œå°†è¾“å…¥åŠ åˆ°è¾“å‡ºä¸Š\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x"
   ],
   "id": "edd6143460bc48f3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention",
   "id": "27a8e4973ba08f6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.073523Z",
     "start_time": "2025-03-17T12:23:07.065393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Attention\n",
    "from torch import einsum, softmax\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b\"\n",
    "                     \" h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)"
   ],
   "id": "ae17307e2f1ce159",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PreNorm",
   "id": "7a2a7cda29003ea7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.109099Z",
     "start_time": "2025-03-17T12:23:07.102642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ],
   "id": "442d84136cf723b2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### NoisyDataClassifier",
   "id": "9c58bde2bdf3ebe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.132105Z",
     "start_time": "2025-03-17T12:23:07.124106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "class UNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=128):\n",
    "        super().__init__()\n",
    "        chs = [1, 64, 128, 256]  # è¾“å…¥é€šé“è°ƒæ•´ä¸º1 (å•é€šé“ç‰¹å¾å›¾)\n",
    "\n",
    "        # æ—¶é—´åµŒå…¥å±‚ (ç§»é™¤æ ‡ç­¾æ¡ä»¶)\n",
    "        self.time_embed = EnhancedTimeEmbedding(time_dim)\n",
    "\n",
    "        # ä¸‹é‡‡æ ·è·¯å¾„ (å¢å¼ºç‰¹å¾æå–)\n",
    "        self.down = nn.ModuleList([\n",
    "            MySequential(\n",
    "                ConditionalBlock(chs[i], chs[i+1], cond_dim=time_dim),\n",
    "                ConditionalBlock(chs[i+1], chs[i+1], cond_dim=time_dim),\n",
    "                Residual(PreNorm(chs[i+1], LinearAttention(chs[i+1])))\n",
    "            ) for i in range(len(chs)-1)\n",
    "        ])\n",
    "\n",
    "        # ä¸­é—´å±‚ (é€‚é…åˆ†ç±»ä»»åŠ¡)\n",
    "        self.mid = MySequential(\n",
    "            ConditionalBlock(chs[-1], chs[-1]*2, cond_dim=time_dim),\n",
    "            Residual(PreNorm(chs[-1]*2, Attention(chs[-1]*2))),\n",
    "            ConditionalBlock(chs[-1]*2, chs[-1], cond_dim=time_dim)\n",
    "        )\n",
    "\n",
    "        # åˆ†ç±»å¤´\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),  # å…¨å±€å¹³å‡æ± åŒ–\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(chs[-1], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        è¾“å…¥:\n",
    "            x: [B, H, W] (å¦‚ [B,24,50])\n",
    "            t: [B,] æ—¶é—´æ­¥\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)  # [B,1,24,50]\n",
    "\n",
    "        # æ—¶é—´åµŒå…¥\n",
    "        t_emb = self.time_embed(t)\n",
    "\n",
    "        skips = []\n",
    "\n",
    "        # ç¼–ç å™¨\n",
    "        for block in self.down:\n",
    "            x = block(x, t_emb)\n",
    "            skips.append(x)\n",
    "            x = F.max_pool2d(x, kernel_size=(2,1))  # ä¿æŒå®½åº¦ä¸å˜\n",
    "\n",
    "        # ä¸­é—´å¤„ç†\n",
    "        x = self.mid(x, t_emb)\n",
    "\n",
    "        # åˆ†ç±»\n",
    "        return self.classifier(x)\n"
   ],
   "id": "6867136b77fada9c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###",
   "id": "1ce081f9fcfc91d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### train_Classifier",
   "id": "325ee603e8195520"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.156410Z",
     "start_time": "2025-03-17T12:23:07.149401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# å‡è®¾ç›¸å…³è‡ªå®šä¹‰æ¨¡å—å·²å®šä¹‰ï¼ˆConditionalBlock, LinearAttentionç­‰ï¼‰\n",
    "# è‹¥å®é™…æ¨¡å—å®šä¹‰ä¸åŒï¼Œéœ€è°ƒæ•´æ­¤å¤„å¯¼å…¥\n",
    "\n",
    "def test_unet_classifier():\n",
    "    # é…ç½®å‚æ•°\n",
    "    batch_size = 4\n",
    "    input_shape = (24, 50)  # é«˜ x å®½\n",
    "    num_classes = 7\n",
    "    time_dim = 128\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # å›ºå®šéšæœºç§å­\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # åˆå§‹åŒ–æ¨¡å‹\n",
    "    model = UNetClassifier(num_classes=num_classes, time_dim=time_dim).to(device)\n",
    "\n",
    "    # ç”Ÿæˆæµ‹è¯•æ•°æ®\n",
    "    x_test = torch.randn(batch_size, *input_shape).to(device)  # [4,24,50]\n",
    "    t_test = torch.randint(0, 1000, (batch_size,)).to(device)  # éšæœºæ—¶é—´æ­¥ [4,]\n",
    "\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_test, t_test)\n",
    "\n",
    "    # éªŒè¯è¾“å‡ºç»´åº¦\n",
    "    expected_shape = (batch_size, num_classes)\n",
    "    assert logits.shape == expected_shape, (\n",
    "        f\"è¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼æœŸæœ›: {expected_shape}, å®é™…: {logits.shape}\"\n",
    "    )\n",
    "\n",
    "    # æ‰“å°æµ‹è¯•ç»“æœ\n",
    "    print(f\"è¾“å…¥å½¢çŠ¶: {x_test.shape}\")\n",
    "    print(f\"æ—¶é—´æ­¥å½¢çŠ¶: {t_test.shape}\")\n",
    "    print(f\"è¾“å‡ºlogitså½¢çŠ¶: {logits.shape}\")\n",
    "    print(\"æµ‹è¯•é€šè¿‡ï¼\")"
   ],
   "id": "eb70a7d0e3c2ce9e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:07.557094Z",
     "start_time": "2025-03-17T12:23:07.171609Z"
    }
   },
   "cell_type": "code",
   "source": "test_unet_classifier()",
   "id": "94eba1bc1df4e44d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥å½¢çŠ¶: torch.Size([4, 24, 50])\n",
      "æ—¶é—´æ­¥å½¢çŠ¶: torch.Size([4])\n",
      "è¾“å‡ºlogitså½¢çŠ¶: torch.Size([4, 7])\n",
      "æµ‹è¯•é€šè¿‡ï¼\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6cc86a49156676fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T12:23:18.114909Z",
     "start_time": "2025-03-17T12:23:07.571095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_conditional_classifier(\n",
    "    x0,                  # åŸå§‹æ•°æ® [N,24,50]\n",
    "    labels,              # æ ‡ç­¾ [N]\n",
    "    num_classes,         # ç±»åˆ«æ•°\n",
    "    num_steps=1000,      # æ‰©æ•£æ€»æ­¥æ•°\n",
    "    batch_size=128,\n",
    "    lr=1e-4,\n",
    "    epochs=500,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_path='best_classifier.pth'\n",
    "):\n",
    "    # æ•°æ®é¢„å¤„ç†\n",
    "    dataset = TensorDataset(x0, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    # åˆå§‹åŒ–æ¨¡å‹\n",
    "    model = UNetClassifier(num_classes=num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    # è®­ç»ƒè®°å½•\n",
    "    best_loss = float('inf')\n",
    "    history = {'train_loss': [], 'acc': []}\n",
    "\n",
    "    print(f\"\\nğŸš€ å¼€å§‹è®­ç»ƒ | è®¾å¤‡: {device}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"æ ·æœ¬æ•°: {len(x0)} | ç±»åˆ«æ•°: {num_classes}\")\n",
    "    print(f\"æ‰¹æ¬¡å¤§å°: {batch_size} | åˆå§‹å­¦ä¹ ç‡: {lr}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for batch_x0, batch_y in pbar:\n",
    "            # æ•°æ®è½¬ç§»è‡³è®¾å¤‡\n",
    "            batch_x0 = batch_x0.to(device)        # [B,24,50]\n",
    "            batch_y = batch_y.to(device)          # [B]\n",
    "\n",
    "            # ç”Ÿæˆéšæœºæ—¶é—´æ­¥\n",
    "            b = batch_x0.size(0)\n",
    "            t = torch.randint(0, num_steps, (b,), device=device)\n",
    "\n",
    "            # å‰å‘åŠ å™ª\n",
    "            noise = torch.randn_like(batch_x0)\n",
    "            sqrt_alpha = torch.sqrt(alphas_prod[t]).view(-1,1,1)\n",
    "            sqrt_one_minus_alpha = torch.sqrt(1 - alphas_prod[t]).view(-1,1,1)\n",
    "            noisy_x = sqrt_alpha * batch_x0 + sqrt_one_minus_alpha * noise\n",
    "\n",
    "            # æ¨¡å‹å‰å‘\n",
    "            logits = model(noisy_x, t)\n",
    "\n",
    "            batch_y = batch_y.squeeze(1)\n",
    "            # è®¡ç®—æŸå¤±\n",
    "            loss = F.cross_entropy(logits, batch_y)\n",
    "\n",
    "            # åå‘ä¼ æ’­\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # ç»Ÿè®¡æŒ‡æ ‡\n",
    "            epoch_loss += loss.item() * b\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total += b\n",
    "\n",
    "            # æ›´æ–°è¿›åº¦æ¡\n",
    "            pbar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'acc': f\"{correct/total:.2%}\"\n",
    "            })\n",
    "\n",
    "        # æ›´æ–°å­¦ä¹ ç‡\n",
    "        scheduler.step()\n",
    "\n",
    "        # è®¡ç®—epochæŒ‡æ ‡\n",
    "        epoch_loss /= len(dataset)\n",
    "        epoch_acc = correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "\n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ | æŸå¤±: {best_loss:.4f}\")\n",
    "\n",
    "        # æ‰“å°epochç»“æœ\n",
    "        print(f\"Epoch {epoch:03d} | \"\n",
    "              f\"Loss: {epoch_loss:.4f} | \"\n",
    "              f\"Acc: {epoch_acc:.2%} | \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    print(\"âœ… è®­ç»ƒå®Œæˆ!\")\n",
    "    return model, history\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    # å‡è®¾å·²åŠ è½½æ•°æ®\n",
    "    # x0: [N,24,50], labels: [N]\n",
    "\n",
    "    model, history = train_conditional_classifier(\n",
    "        x0=x0,\n",
    "        labels=labels,\n",
    "        num_classes=num_classes,\n",
    "        num_steps=200,\n",
    "        batch_size=64,\n",
    "        epochs=200,\n",
    "        save_path='best_noisy_classifier.pth'\n",
    "    )\n"
   ],
   "id": "25ab5158e53825ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ å¼€å§‹è®­ç»ƒ | è®¾å¤‡: cpu\n",
      "---------------------------------------\n",
      "æ ·æœ¬æ•°: 2368 | ç±»åˆ«æ•°: 7\n",
      "æ‰¹æ¬¡å¤§å°: 64 | åˆå§‹å­¦ä¹ ç‡: 0.0001\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   5%|â–Œ         | 2/37 [00:10<02:59,  5.14s/it, loss=2.02, acc=17.97%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 111\u001B[39m\n\u001B[32m    106\u001B[39m \u001B[38;5;66;03m# ä½¿ç”¨ç¤ºä¾‹\u001B[39;00m\n\u001B[32m    107\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    108\u001B[39m     \u001B[38;5;66;03m# å‡è®¾å·²åŠ è½½æ•°æ®\u001B[39;00m\n\u001B[32m    109\u001B[39m     \u001B[38;5;66;03m# x0: [N,24,50], labels: [N]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m111\u001B[39m     model, history = \u001B[43mtrain_conditional_classifier\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    112\u001B[39m \u001B[43m        \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m=\u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    113\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    114\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    116\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    117\u001B[39m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[43m        \u001B[49m\u001B[43msave_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbest_noisy_classifier.pth\u001B[39;49m\u001B[33;43m'\u001B[39;49m\n\u001B[32m    119\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 66\u001B[39m, in \u001B[36mtrain_conditional_classifier\u001B[39m\u001B[34m(x0, labels, num_classes, num_steps, batch_size, lr, epochs, device, save_path)\u001B[39m\n\u001B[32m     64\u001B[39m \u001B[38;5;66;03m# åå‘ä¼ æ’­\u001B[39;00m\n\u001B[32m     65\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m66\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     67\u001B[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001B[32m1.0\u001B[39m)\n\u001B[32m     68\u001B[39m optimizer.step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    617\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    618\u001B[39m         Tensor.backward,\n\u001B[32m    619\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    624\u001B[39m         inputs=inputs,\n\u001B[32m    625\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m626\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    628\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    821\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    822\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m823\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    824\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    825\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    826\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    827\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
