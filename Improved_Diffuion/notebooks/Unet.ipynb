{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-15T06:46:49.411894Z",
     "start_time": "2025-03-15T06:46:46.750658Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import einsum"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T06:46:49.426114Z",
     "start_time": "2025-03-15T06:46:49.413648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def exists(x):\n",
    "    return x is not None"
   ],
   "id": "d2cdd3bddea4736e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T06:46:49.629728Z",
     "start_time": "2025-03-15T06:46:49.621321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ],
   "id": "1e7fa095cb69f1d4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T06:46:49.645442Z",
     "start_time": "2025-03-15T06:46:49.637706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sinusoidal_embedding(t, dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t: 时间步张量 [batch_size, ]\n",
    "        dim: 嵌入维度\n",
    "    Returns:\n",
    "        嵌入向量 [batch_size, dim]\n",
    "    \"\"\"\n",
    "    device = t.device\n",
    "    half_dim = dim // 2\n",
    "    emb = torch.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "    emb = t.float()[:, None] * emb[None, :]  # [batch_size, half_dim]\n",
    "\n",
    "    # 拼接正弦和余弦分量\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "\n",
    "    # 处理奇数维度情况\n",
    "    if dim % 2 == 1:\n",
    "        emb = F.pad(emb, (0, 1), mode='constant')\n",
    "\n",
    "    return emb\n"
   ],
   "id": "f744d6acc779bfe5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T06:46:49.661035Z",
     "start_time": "2025-03-15T06:46:49.653325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConditionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=256, label_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(time_dim, time_dim*4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim*4, time_dim*4)\n",
    "        )\n",
    "        self.label_embed = nn.Embedding(num_classes, label_dim)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(time_dim*4 + label_dim, time_dim*2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim*2, time_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        # t: [B,] 时间步\n",
    "        # y: [B,] 标签\n",
    "        t_emb = sinusoidal_embedding(t, self.time_embed[0].in_features)\n",
    "        t_emb = self.time_embed(t_emb)  # [B, time_dim]\n",
    "\n",
    "        l_emb = self.label_embed(y).squeeze(1)     # [B, label_dim]\n",
    "\n",
    "        # 融合时间与标签信息\n",
    "        combined = torch.cat([t_emb, l_emb], dim=1)\n",
    "        return self.fusion(combined)    # [B, time_dim]"
   ],
   "id": "8a9c56ab99ec4153",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T06:46:49.675933Z",
     "start_time": "2025-03-15T06:46:49.669343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MySequential(nn.Sequential):\n",
    "    def forward(self, x, t_emb):\n",
    "        for module in self:\n",
    "            if isinstance(module, ConditionalBlock):  # 仅对特定模块传参\n",
    "                x = module(x, t_emb)\n",
    "            else:  # 其他模块按默认方式处理\n",
    "                x = module(x)\n",
    "        return x"
   ],
   "id": "7d364ce62bf28735",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T06:46:49.694309Z",
     "start_time": "2025-03-15T06:46:49.684462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b\"\n",
    "                     \" h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)"
   ],
   "id": "88411a3441c4b923",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T06:46:49.725042Z",
     "start_time": "2025-03-15T06:46:49.703590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from einops import rearrange\n",
    "\n",
    "class ConditionalBlock(nn.Module):\n",
    "    \"\"\"基于你原有MyBlock改造的条件版本\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, cond_dim, mult=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cond_dim: 条件向量的维度 (time+label的融合维度)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 修改后的条件投影层（移除偏置项）验证条件注入的有效性\n",
    "        self.cond_mlp = nn.Sequential(\n",
    "            nn.Linear(cond_dim, out_ch*2, bias=False),  # 关键修改：bias=False\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        # 保持原有卷积结构\n",
    "        self.ds_conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.GroupNorm(1, out_ch),\n",
    "            nn.Conv2d(out_ch, out_ch * mult, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.GroupNorm(1, out_ch * mult),\n",
    "            nn.Conv2d(out_ch * mult, out_ch, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond_emb):\n",
    "        \"\"\"输入变化：t_emb → cond_emb (融合时间+标签的条件向量)\"\"\"\n",
    "        h = self.ds_conv(x)\n",
    "\n",
    "        # 条件注入 (scale and shift)\n",
    "        scale, shift = self.cond_mlp(cond_emb).chunk(2, dim=1)  # [B, 2*out_ch] → [B, out_ch], [B, out_ch]\n",
    "        h = h * (1 + scale[:, :, None, None])  # 缩放\n",
    "        h = h + shift[:, :, None, None]        # 偏移\n",
    "\n",
    "        h = self.conv(h)\n",
    "        return h + self.res_conv(x)  # 保持原有残差连接\n"
   ],
   "id": "2fb693399fd4d768",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T06:46:49.740476Z",
     "start_time": "2025-03-15T06:46:49.733680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 残差模块，将输入加到输出上\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x"
   ],
   "id": "3647469743779291",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T06:46:49.782224Z",
     "start_time": "2025-03-15T06:46:49.766360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConditionalDiffusionUNet(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=128, label_dim=64):\n",
    "        super().__init__()\n",
    "        chs = [1, 64, 128, 256]\n",
    "\n",
    "        # 替换为条件嵌入层\n",
    "        self.cond_embed = ConditionalEmbedding(\n",
    "            num_classes=num_classes,\n",
    "            time_dim=time_dim,\n",
    "            label_dim=label_dim\n",
    "        )\n",
    "        cond_dim = time_dim  # 条件向量的总维度\n",
    "\n",
    "        # 下采样路径（修改所有MyBlock的cond_dim）\n",
    "        self.down = nn.ModuleList([\n",
    "            MySequential(\n",
    "                ConditionalBlock(chs[i], chs[i+1], cond_dim=cond_dim),\n",
    "                ConditionalBlock(chs[i+1], chs[i+1], cond_dim=cond_dim),\n",
    "                Residual(PreNorm(chs[i+1], LinearAttention(chs[i+1])))\n",
    "            ) for i in range(len(chs)-1)\n",
    "        ])\n",
    "\n",
    "        # 中间层\n",
    "        self.mid = MySequential(\n",
    "            ConditionalBlock(chs[-1], chs[-1], cond_dim=cond_dim),\n",
    "            Residual(PreNorm(chs[-1], Attention(chs[-1]))),\n",
    "            ConditionalBlock(chs[-1], chs[-1], cond_dim=cond_dim)\n",
    "        )\n",
    "\n",
    "        # 上采样路径\n",
    "        self.up = nn.ModuleList([\n",
    "            MySequential(\n",
    "                ConditionalBlock(chs[i+1]*2, chs[i], cond_dim=cond_dim),\n",
    "                ConditionalBlock(chs[i], chs[i], cond_dim=cond_dim),\n",
    "                Residual(PreNorm(chs[i], LinearAttention(chs[i])))\n",
    "            ) for i in reversed(range(len(chs)-1))\n",
    "        ])\n",
    "\n",
    "        self.final = nn.Conv2d(chs[0], 2, 1)\n",
    "\n",
    "    def forward(self, x, t, y):\n",
    "        cond_emb = self.cond_embed(t, y)  # 获取融合条件向量\n",
    "        skips = []\n",
    "\n",
    "        # 编码器（传递cond_emb）\n",
    "        for block in self.down:\n",
    "            x = block(x, cond_emb)\n",
    "            skips.append(x)\n",
    "            x = F.max_pool2d(x, kernel_size=(2,1))\n",
    "\n",
    "        # 中间处理\n",
    "        x = self.mid(x, cond_emb)\n",
    "\n",
    "        # 解码器\n",
    "        for i, block in enumerate(self.up):\n",
    "            x = F.interpolate(x, scale_factor=(2,1), mode='nearest')\n",
    "            x = torch.cat([x, skips[-(i+1)]], dim=1)\n",
    "            x = block(x, cond_emb)\n",
    "\n",
    "        return self.final(x)"
   ],
   "id": "695462d69ac982f6",
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
