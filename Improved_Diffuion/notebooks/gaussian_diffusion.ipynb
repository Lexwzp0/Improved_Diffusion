{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from data.pyg_dataToGraph import DataToGraph\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from .losses import normal_kl, discretized_gaussian_log_likelihood"
   ],
   "id": "6d195294650cd7cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))"
   ],
   "id": "704adf8d441f8d29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    res = torch.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res.expand(broadcast_shape)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "以下是你的 `GaussianDiffusion` 类中关键参数的数学公式解析，结合代码实现与扩散模型理论，整理成清晰表格：\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 时间步与基础参数**\n",
    "| 代码变量                  | 数学符号/公式                                                                         | 物理意义                          |\n",
    "|---------------------------|---------------------------------------------------------------------------------|----------------------------------|\n",
    "| `self.t`                  | \\($ t \\in [0, 1] $\\)                                                            | 归一化的时间步（从 0 到 1 线性分布） |\n",
    "| `self.betas`              | \\($ \\beta_t = \\frac{\\cos^2(\\frac{\\pi}{2} t)}{\\max(\\cos^2(\\frac{\\pi}{2} t))} $\\) | 余弦调度的噪声系数（调整到 1e-5 ~ 5e-3） |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 前向过程参数**\n",
    "| 代码变量                  | 数学公式                                                    | 物理意义                          |\n",
    "|---------------------------|---------------------------------------------------------|----------------------------------|\n",
    "| `self.alphas`             | \\($ \\alpha_t = 1 - \\beta_t $\\)                          | 单步保留信号系数                  |\n",
    "| `self.alphas_cumprod`     | \\($ \\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s $\\)         | 累积信号保留系数（控制噪声强度）    |\n",
    "| `self.alphas_cumprod_prev`| \\($ \\bar{\\alpha}_{t-1} = \\prod_{s=1}^{t-1} \\alpha_s $\\) | 前一步的累积信号保留系数          |\n",
    "| `self.sqrt_alphas_cumprod`| \\($ \\sqrt{\\bar{\\alpha}_t} $\\)                           | 用于计算加噪均值的系数            |\n",
    "| `self.sqrt_one_minus_alphas_cumprod` | \\($ \\sqrt{1 - \\bar{\\alpha}_t} $\\)                       | 用于计算加噪噪声权重的系数        |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 反向过程参数**\n",
    "| 代码变量                  | 数学公式                                                                              | 物理意义                      |\n",
    "|---------------------------|-----------------------------------------------------------------------------------|---------------------------|\n",
    "| `self.posterior_mean_coef1` | \\($ c_1 = \\frac{\\beta_t \\sqrt{\\bar{\\alpha}_{t-1}}}{1 - \\bar{\\alpha}_t} $\\)        | 反向过程均值计算中 \\($ x_0 $\\) 的权重 |\n",
    "| `self.posterior_mean_coef2` | \\($ c_2 = \\frac{\\sqrt{\\alpha_t} (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} $\\) | 反向过程均值计算中 \\($ x_t $\\) 的权重 |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 关键公式对应代码**\n",
    "#### **(1) 前向加噪过程**\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon\n",
    "$$\n",
    "对应代码：\n",
    "```python\n",
    "x_t = (\n",
    "    _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "    + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    ")\n",
    "```\n",
    "\n",
    "#### **(2) 反向后验均值**\n",
    "$$\n",
    "\\tilde{\\mu}_t = c_1 \\cdot x_0 + c_2 \\cdot x_t\n",
    "$$\n",
    "对应代码：\n",
    "```python\n",
    "posterior_mean = (\n",
    "    _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "    + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    ")\n",
    "```\n",
    "\n",
    "---"
   ],
   "id": "dfe5ec694a672145"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:20:03.194497700Z",
     "start_time": "2025-03-13T11:36:21.245199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GaussianDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_steps,\n",
    "        #model,\n",
    "        #classifier,\n",
    "        classifier_scale = 1.0\n",
    "        #model_mean_type = \"epsilon\",\n",
    "        #model_var_type,\n",
    "        #loss_type,\n",
    "        #rescale_timesteps=False\n",
    "        ):\n",
    "        self.num_steps = num_steps\n",
    "        #self.model = model\n",
    "        #self.classifier = classifier\n",
    "        self.classifier_scale = classifier_scale\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # # 生成时间步的序列\n",
    "        # self.t = torch.linspace(0, 1, num_steps + 1)  # 主要时间步范围从 0 到 1\n",
    "        #\n",
    "        # # 使用余弦调度生成 betas\n",
    "        # self.betas = torch.cos(torch.pi / 2.0 * self.t) ** 2  # 余弦平方函数\n",
    "        # self.betas = self.betas / self.betas.max()  # 归一化到 0-1 范围\n",
    "        # self.betas = torch.flip(self.betas, [0])  # 反转顺序，以确保从小到大递增\n",
    "        # self.betas = torch.clamp(self.betas, min=1e-5, max=0.5e-2)  # 调整范围到 (1e-5, 0.5e-2)\n",
    "\n",
    "        # 生成betas序列\n",
    "        self.betas = self._cosine_beta_schedule_with_offset(\n",
    "            num_steps=num_steps,\n",
    "            max_beta=0.999,      # 可调节参数\n",
    "            offset=0.008         # Improved DDPM 标准偏移量\n",
    "        )\n",
    "\n",
    "        # 计算 alpha , alpha_prod , alpha_prod_previous , alpha_bar_sqrt 等变量的值\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)  # 累积连乘\n",
    "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1]).float(), self.alphas_cumprod[:-1]], 0)  # p means previous\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.log_one_minus_alphas_cumprod = torch.log(1 - self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)\n",
    "        # self.posterior_log_variance_clipped = torch.log(\n",
    "        #     torch.append(self.posterior_variance[1], self.posterior_variance[1:])\n",
    "        # )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.cat([\n",
    "                self.posterior_variance[1:2],  # 保持维度一致\n",
    "                self.posterior_variance[1:]\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        # 计算后验参数coef1和coef2\n",
    "        self.posterior_mean_coef1 = (\n",
    "            self.betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * np.sqrt(self.alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "        # 将超参数也移动到GPU\n",
    "        self.betas = self.betas.to(self.device)\n",
    "        self.alphas = self.alphas.to(self.device)\n",
    "        self.alphas_cumprod = self.alphas_cumprod.to(self.device)\n",
    "        self.alphas_cumprod_prev = self.alphas_cumprod_prev.to(self.device)\n",
    "        self.sqrt_alphas_cumprod = self.sqrt_alphas_cumprod.to(self.device)\n",
    "        self.posterior_variance = self.posterior_variance.to(self.device)\n",
    "        self.log_one_minus_alphas_cumprod = self.log_one_minus_alphas_cumprod.to(self.device)\n",
    "        self.sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod.to(self.device)\n",
    "        self.posterior_mean_coef1 = self.posterior_mean_coef1.to(self.device)\n",
    "        self.posterior_mean_coef2 = self.posterior_mean_coef2.to(self.device)\n",
    "\n",
    "    def _cosine_beta_schedule_with_offset(self, num_steps, max_beta=0.999, offset=0.008):\n",
    "        \"\"\"Improved DDPM 余弦调度核心函数\"\"\"\n",
    "        # 生成连续时间点 (包括端点)\n",
    "        t = torch.linspace(0, 1, num_steps + 1)\n",
    "\n",
    "        # 计算 alpha_bar (累积乘积)\n",
    "        alpha_bars = torch.cos((t + offset) / (1 + offset) * torch.pi / 2) ** 2\n",
    "\n",
    "        # 计算 beta 值\n",
    "        betas = []\n",
    "        for i in range(num_steps):\n",
    "            beta = 1 - (alpha_bars[i+1] / alpha_bars[i])\n",
    "            betas.append(min(beta.item(), max_beta))\n",
    "\n",
    "        # 强制单调递增\n",
    "        for i in range(1, len(betas)):\n",
    "            if betas[i] < betas[i-1]:\n",
    "                betas[i] = betas[i-1]\n",
    "\n",
    "        return torch.tensor(betas, dtype=torch.float32)\n",
    "\n",
    "    def _predict_xstart_from_xprev(self, x_t, t, xprev):\n",
    "        assert x_t.shape == xprev.shape\n",
    "        return (  # (xprev - coef2*x_t) / coef1\n",
    "            _extract_into_tensor(1.0 / self.posterior_mean_coef1, t, x_t.shape) * xprev\n",
    "            - _extract_into_tensor(\n",
    "                self.posterior_mean_coef2 / self.posterior_mean_coef1, t, x_t.shape\n",
    "            )\n",
    "            * x_t\n",
    "        )\n",
    "\n",
    "    def _vb_terms_bpd(\n",
    "            self, model, x_start, x_t, t, batch_labels = None, clip_denoised = True\n",
    "    ):\n",
    "        true_mean, _, true_log_variance_clipped = self.q_posterior_mean_variance(\n",
    "            x_start=x_start, x_t=x_t, t=t\n",
    "        )\n",
    "        out = self.p_mean_variance(\n",
    "            model, x_t, t, batch_labels, clip_denoised=clip_denoised\n",
    "        )\n",
    "        kl = normal_kl(\n",
    "            true_mean, true_log_variance_clipped, out[\"mean\"], out[\"log_variance\"]\n",
    "        )\n",
    "        kl = mean_flat(kl) / np.log(2.0)\n",
    "\n",
    "        decoder_nll = -discretized_gaussian_log_likelihood(\n",
    "            x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"]\n",
    "        )\n",
    "        assert decoder_nll.shape == x_start.shape\n",
    "        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n",
    "\n",
    "        # At the first timestep return the decoder NLL,\n",
    "        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n",
    "        output = torch.where((t == 0), decoder_nll, kl)\n",
    "        return {\"output\": output, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def cond_fn(self, x, t, y):\n",
    "        \"\"\"分类器梯度计算函数\"\"\"\n",
    "        assert y is not None\n",
    "        with torch.enable_grad():\n",
    "            x_in = x.detach().requires_grad_(True)\n",
    "            logits = self.classifier(x_in, t)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            selected = log_probs[range(len(logits)), y.view(-1)]\n",
    "            return torch.autograd.grad(selected.sum(), x_in)[0] * self.classifier_scale\n",
    "\n",
    "    def condition_mean(self, p_mean_var, x, t, batch_y):\n",
    "        gradient = self.cond_fn(x, t, batch_y)\n",
    "        new_mean = (\n",
    "            p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n",
    "        )\n",
    "        return new_mean\n",
    "\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        # 前向计算mean和variance，根据前向扩散公式推导可得mean和variance\n",
    "        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        variance = extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def q_sample(self, x_start, t, noise = None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        assert noise.shape == x_start.shape\n",
    "\n",
    "        return (\n",
    "                extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "                + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        assert x_start.shape == x_t.shape\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(\n",
    "            self.posterior_log_variance_clipped, t, x_t.shape\n",
    "        )\n",
    "        assert (\n",
    "            posterior_mean.shape[0]\n",
    "            == posterior_variance.shape[0]\n",
    "            == posterior_log_variance_clipped.shape[0]\n",
    "            == x_start.shape[0]\n",
    "        )\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(\n",
    "            self, model, x, t, batch_labels = None, clip_denoised=True, denoised_fn=None,\n",
    "    ):\n",
    "        B, C = x.shape[:2]\n",
    "        assert t.shape == (B,)\n",
    "        model_output = model(x, t, batch_labels)\n",
    "\n",
    "        assert model_output.shape == (B, C * 2, *x.shape[2:])\n",
    "        model_output, model_var_values = torch.split(model_output, C, dim=1)\n",
    "        min_log = extract(self.posterior_log_variance_clipped, t, x.shape)\n",
    "        max_log = extract(torch.log(self.betas), t, x.shape)\n",
    "        frac = (model_var_values + 1) / 2\n",
    "        model_log_variance = frac * max_log + (1 - frac) * min_log\n",
    "        model_variance = torch.exp(model_log_variance)\n",
    "\n",
    "        def process_xstart(x):\n",
    "            if denoised_fn is not None:\n",
    "                x = denoised_fn(x)\n",
    "            if clip_denoised:\n",
    "                return x.clamp(-1, 1)\n",
    "            return x\n",
    "\n",
    "        pred_xstart = process_xstart(\n",
    "                self._predict_xstart_from_xprev(x_t=x, t=t, xprev=model_output)\n",
    "            )\n",
    "        model_mean = model_output\n",
    "\n",
    "        assert (\n",
    "            model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n",
    "        )\n",
    "        return {\n",
    "            \"mean\": model_mean,\n",
    "            \"variance\": model_variance,\n",
    "            \"log_variance\": model_log_variance,\n",
    "            \"pred_xstart\": pred_xstart,\n",
    "        }\n",
    "\n",
    "    def p_sample(self,\n",
    "                 model,\n",
    "                 x,\n",
    "                 t,\n",
    "                 batch_labels,\n",
    "                 clip_denoised = True,\n",
    "                 denoised_fn = None,\n",
    "                 ):\n",
    "        out = self.p_mean_variance(model, x, t, batch_labels, clip_denoised, denoised_fn)\n",
    "        noise = torch.randn_like(x)\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )\n",
    "        out[\"mean\"] = self.condition_mean(\n",
    "             out, x, t, batch_labels\n",
    "        )\n",
    "        sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise\n",
    "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def p_sample_loop_progressive(\n",
    "            self,\n",
    "            shape,\n",
    "            batch_labels,\n",
    "            noise = None,\n",
    "            clip_denoised=True,\n",
    "            denoised_fn=None,\n",
    "            progress=False,\n",
    "    ):\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None:\n",
    "            img = noise\n",
    "        else:\n",
    "            img = torch.randn(*shape, device=self.device)\n",
    "        indices = list(range(self.num_steps))[::-1]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            t = torch.tensor([i] * shape[0], device=self.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.p_sample(\n",
    "                    img,\n",
    "                    t,\n",
    "                    batch_labels,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn,\n",
    "                )\n",
    "                yield out\n",
    "                img = out[\"sample\"]\n",
    "\n",
    "    def p_sample_loop(\n",
    "            self,\n",
    "            shape,\n",
    "            batch_labels,\n",
    "            noise=None,\n",
    "            clip_denoised=True,\n",
    "            denoised_fn=None,\n",
    "            progress=False,\n",
    "    ):\n",
    "        final = None\n",
    "        for sample in self.p_sample_loop_progressive(\n",
    "            shape,\n",
    "            batch_labels,\n",
    "            noise = noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            progress=progress,\n",
    "        ):\n",
    "            final = sample\n",
    "        return final[\"sample\"]\n",
    "\n",
    "    def training_losses(self, model, x_start, t, batch_labels, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        x_t = self.q_sample(x_start, t, noise=noise)\n",
    "\n",
    "        terms = {}\n",
    "        model_output = model(x_t, t, batch_labels)  # 完整模型输出 [B, 2C, ...]\n",
    "\n",
    "        B, C = x_t.shape[:2]  # 修改点1：确保获取正确的通道维度\n",
    "        assert model_output.shape == (B, C * 2, *x_t.shape[2:])\n",
    "\n",
    "        # 分割噪声预测和方差预测\n",
    "        model_output, model_var_values = torch.split(model_output, C, dim=1)\n",
    "\n",
    "        # 关键修改点2：创建冻结梯度后的拼接输出\n",
    "        frozen_out = torch.cat([model_output.detach(), model_var_values], dim=1)  # 冻结噪声预测梯度\n",
    "\n",
    "        # 修改点3：使用 frozen_out 代替原始模型\n",
    "        terms[\"vb\"] = self._vb_terms_bpd(\n",
    "            model=lambda *args, r=frozen_out: r,  # 直接返回预计算的 frozen_out\n",
    "            x_start=x_start,\n",
    "            x_t=x_t,\n",
    "            t=t,\n",
    "            batch_labels=batch_labels,  # 注意：需要确认 _vb_terms_bpd 是否支持这个参数\n",
    "            clip_denoised=False\n",
    "        )[\"output\"]\n",
    "\n",
    "        # 强制使用 PREVIOUS_X 目标\n",
    "        target, _ = self.q_posterior_mean_variance(x_start, x_t, t)\n",
    "        assert model_output.shape == target.shape == x_start.shape\n",
    "        terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n",
    "        terms[\"loss\"] = terms[\"mse\"] + terms[\"vb\"]\n",
    "\n",
    "        return terms\n"
   ],
   "id": "5796eefb3e9fff4a",
   "outputs": [],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
