{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch import einsum\n",
    "from einops import rearrange"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MySequential(nn.Sequential):\n",
    "    def forward(self, x, t_emb):\n",
    "        for module in self:\n",
    "            if isinstance(module, ConditionalBlock):  # 仅对特定模块传参\n",
    "                x = module(x, t_emb)\n",
    "            else:  # 其他模块按默认方式处理\n",
    "                x = module(x)\n",
    "        return x"
   ],
   "id": "38e5d65c78279338"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sinusoidal_embedding(t, dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t: 时间步张量 [batch_size, ]\n",
    "        dim: 嵌入维度\n",
    "    Returns:\n",
    "        嵌入向量 [batch_size, dim]\n",
    "    \"\"\"\n",
    "    device = t.device\n",
    "    half_dim = dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "    emb = t.float()[:, None] * emb[None, :]  # [batch_size, half_dim]\n",
    "\n",
    "    # 拼接正弦和余弦分量\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "\n",
    "    # 处理奇数维度情况\n",
    "    if dim % 2 == 1:\n",
    "        emb = F.pad(emb, (0, 1), mode='constant')\n",
    "\n",
    "    return emb\n",
    "\n",
    "\n",
    "class ConditionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=256, label_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim)\n",
    "        )\n",
    "        self.label_embed = nn.Embedding(num_classes, label_dim)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(time_dim + label_dim, time_dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 2, time_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        # t: [B,] 时间步\n",
    "        # y: [B,] 标签\n",
    "        t_emb = sinusoidal_embedding(t, self.time_embed[0].in_features)\n",
    "        t_emb = self.time_embed(t_emb)  # [B, time_dim]\n",
    "\n",
    "        l_emb = self.label_embed(y).squeeze(1)  # [B, label_dim]\n",
    "\n",
    "        # 融合时间与标签信息\n",
    "        combined = torch.cat([t_emb, l_emb], dim=1)\n",
    "        return self.fusion(combined)  # [B, time_dim]"
   ],
   "id": "67ee1c08f89fca22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EnhancedTimeEmbedding(nn.Module):\n",
    "    \"\"\"增强时间嵌入（添加多层感知）\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.GELU(),\n",
    "            #nn.SiLU(),\n",
    "            nn.Linear(dim*4, dim),\n",
    "            #nn.SiLU(),\n",
    "            #nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return self.embed(emb)"
   ],
   "id": "e975594d0dade385"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ConditionalBlock(nn.Module):\n",
    "    \"\"\"基于你原有MyBlock改造的条件版本\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, cond_dim, mult=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cond_dim: 条件向量的维度 (time+label的融合维度)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 修改后的条件投影层（移除偏置项）验证条件注入的有效性\n",
    "        self.cond_mlp = nn.Sequential(\n",
    "            nn.Linear(cond_dim, out_ch*2, bias=False),  # 关键修改：bias=False\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        # 保持原有卷积结构\n",
    "        self.ds_conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.GroupNorm(1, out_ch),\n",
    "            nn.Conv2d(out_ch, out_ch * mult, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.GroupNorm(1, out_ch * mult),\n",
    "            nn.Conv2d(out_ch * mult, out_ch, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond_emb):\n",
    "        \"\"\"输入变化：t_emb → cond_emb (融合时间+标签的条件向量)\"\"\"\n",
    "        h = self.ds_conv(x)\n",
    "\n",
    "        # 条件注入 (scale and shift)\n",
    "        scale, shift = self.cond_mlp(cond_emb).chunk(2, dim=1)  # [B, 2*out_ch] → [B, out_ch], [B, out_ch]\n",
    "        h = h * (1 + scale[:, :, None, None])  # 缩放\n",
    "        h = h + shift[:, :, None, None]        # 偏移\n",
    "\n",
    "        h = self.conv(h)\n",
    "        return h + self.res_conv(x)  # 保持原有残差连接"
   ],
   "id": "24e392e846d32cbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 残差模块，将输入加到输出上\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x"
   ],
   "id": "4e133cae823371d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b\"\n",
    "                     \" h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)"
   ],
   "id": "17424019d84fa64d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ],
   "id": "fb0de9a12d0d7f16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn.functional as F\n",
    "class UNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, time_dim=128):\n",
    "        super().__init__()\n",
    "        chs = [1, 64, 128, 256]  # 输入通道调整为1 (单通道特征图)\n",
    "\n",
    "        # 时间嵌入层 (移除标签条件)\n",
    "        self.time_embed = EnhancedTimeEmbedding(time_dim)\n",
    "\n",
    "        # 下采样路径 (增强特征提取)\n",
    "        self.down = nn.ModuleList([\n",
    "            MySequential(\n",
    "                ConditionalBlock(chs[i], chs[i+1], cond_dim=time_dim),\n",
    "                ConditionalBlock(chs[i+1], chs[i+1], cond_dim=time_dim),\n",
    "                Residual(PreNorm(chs[i+1], LinearAttention(chs[i+1])))\n",
    "            ) for i in range(len(chs)-1)\n",
    "        ])\n",
    "\n",
    "        # 中间层 (适配分类任务)\n",
    "        self.mid = MySequential(\n",
    "            ConditionalBlock(chs[-1], chs[-1]*2, cond_dim=time_dim),\n",
    "            Residual(PreNorm(chs[-1]*2, Attention(chs[-1]*2))),\n",
    "            ConditionalBlock(chs[-1]*2, chs[-1], cond_dim=time_dim)\n",
    "        )\n",
    "\n",
    "        # 分类头\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),  # 全局平均池化\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(chs[-1], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        输入:\n",
    "            x: [B,C * 2, H, W] (如 [B,2,24,50])\n",
    "            t: [B,] 时间步\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)  # [B,1,24,50]\n",
    "\n",
    "        # 时间嵌入\n",
    "        t_emb = self.time_embed(t)\n",
    "\n",
    "        skips = []\n",
    "\n",
    "        # 编码器\n",
    "        for block in self.down:\n",
    "            x = block(x, t_emb)\n",
    "            skips.append(x)\n",
    "            x = F.max_pool2d(x, kernel_size=(2,1))  # 保持宽度不变\n",
    "\n",
    "        # 中间处理\n",
    "        x = self.mid(x, t_emb)\n",
    "\n",
    "        # 分类\n",
    "        return self.classifier(x)\n"
   ],
   "id": "c756083eb95991bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
