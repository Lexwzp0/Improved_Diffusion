{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:42:54.591190Z",
     "start_time": "2025-03-20T10:42:54.582757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.pyg_dataToGraph import DataToGraph\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "from src.gaussian_diffusion import GaussianDiffusion\n",
    "from src.classifier import UNetClassifier"
   ],
   "id": "79aa29292a7e6e74",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-20T10:42:55.049010Z",
     "start_time": "2025-03-20T10:42:54.661253Z"
    }
   },
   "source": [
    "# TODO Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "dataset = DataToGraph(\n",
    "    raw_data_path='../data/raw',\n",
    "    dataset_name='TFF' + '.mat')  # Ê†ºÂºè: [(graph,label),...,(graph,label)]\n",
    "\n",
    "input_dim = dataset[0].x.size(1)\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# ÊèêÂèñÊâÄÊúâÁöÑxÂíåy\n",
    "x0 = []\n",
    "labels = []\n",
    "\n",
    "for data in dataset:\n",
    "    # ÊèêÂèñx (ÂΩ¢Áä∂‰∏∫ [num_nodes, input_dim])\n",
    "    # ‰ΩÜÊòØ‰Ω†ÊèêÂà∞dataset.xÁöÑÂΩ¢Áä∂ÊòØ [24,50]ÔºåËøôÂèØËÉΩÊòØ‰∏Ä‰∏™ÂõæÁöÑxÁâπÂæÅÁü©Èòµ\n",
    "    x0.append(data.x)\n",
    "    # ÊèêÂèñyÔºàÊ†áÈáèÊ†áÁ≠æÔºâ\n",
    "    labels.append(data.y)\n",
    "\n",
    "# Â∞ÜÂàóË°®ËΩ¨Êç¢‰∏∫Âº†Èáè\n",
    "x0 = torch.stack(x0).unsqueeze(1)  # ÂΩ¢Áä∂ [num_samples, 1, 24, 50]\n",
    "\n",
    "labels = torch.stack(labels)  # ÂΩ¢Áä∂ [num_samples]\n",
    "\n",
    "print(num_classes)\n",
    "print(\"X0 shape:\", x0.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "X0 shape: torch.Size([2368, 1, 24, 50])\n",
      "Labels shape: torch.Size([2368, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:42:55.401770Z",
     "start_time": "2025-03-20T10:42:55.082047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Labels shape:\", labels.shape)\n",
    "# Â∞ÜÊï∞ÊçÆ‰º†ËæìÂà∞GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ÂàùÂßãÂåñÊ®°Âûã\n",
    "classifier = UNetClassifier(num_classes)\n",
    "# ÂáÜÂ§áÊï∞ÊçÆÈõÜ\n",
    "dataset = TensorDataset(x0, labels)  # x0: [N,24,50], labels: [N]\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=64,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True)"
   ],
   "id": "5c18a4e0d5e4d78f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: torch.Size([2368, 1])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:42:55.491305Z",
     "start_time": "2025-03-20T10:42:55.469834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 1000\n",
    "lr = 3e-4\n",
    "grad_clip = 1.0\n",
    "save_interval = 50  # ÊØè50‰∏™epoch‰øùÂ≠ò‰∏ÄÊ¨°Ê®°Âûã\n",
    "\n",
    "# ÂàùÂßãÂåñÊâ©Êï£Ê®°Âûã\n",
    "diffusion = GaussianDiffusion(num_steps=64)\n",
    "\n",
    "# ÂàùÂßãÂåñ‰ºòÂåñÂô®ÂíåÂ≠¶‰π†ÁéáË∞ÉÂ∫¶Âô®\n",
    "optimizer = AdamW(classifier.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "# Â∞ÜÊï∞ÊçÆËΩ¨ÁßªÂà∞GPU\n",
    "x0 = x0.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "best_loss = float('inf')\n",
    "train_losses = []\n"
   ],
   "id": "da18549940860d32",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:48:45.822585Z",
     "start_time": "2025-03-20T10:42:55.608472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "acc_history = []\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for batch_idx, (x_batch, label_batch) in enumerate(pbar):\n",
    "            # Êï∞ÊçÆËΩ¨ÁßªÂà∞ËÆæÂ§á\n",
    "            x_batch = x_batch.to(device)  # [B, 1, 24, 50]\n",
    "            label_batch = label_batch.to(device)  # [B]\n",
    "            label_batch = label_batch.squeeze(1)  # Êñ∞Â¢û‰ª£Á†Å\n",
    "            # ÈöèÊú∫ÈááÊ†∑Êó∂Èó¥Ê≠• (ÂÖ≥ÈîÆÊ≠•È™§!)\n",
    "            B = x_batch.size(0)\n",
    "            t = torch.randint(0, diffusion.num_steps, (B,), device=device).long()\n",
    "\n",
    "            # ‰ΩøÁî®Êâ©Êï£Ê®°ÂûãÂä†Âô™ (Ê†∏ÂøÉ‰øÆÊîπÁÇπ)\n",
    "            noisy_batch = diffusion.q_sample(x_start=x_batch, t=t)\n",
    "\n",
    "            # ÂàÜÁ±ªÂô®ÂâçÂêë‰º†Êí≠\n",
    "            logits = classifier(noisy_batch, t)  # ÂÅáËÆæÂàÜÁ±ªÂô®Êé•ÂèóÊó∂Èó¥Ê≠•ËæìÂÖ•\n",
    "\n",
    "            # ËÆ°ÁÆóÊçüÂ§±\n",
    "            loss = F.cross_entropy(logits, label_batch)\n",
    "\n",
    "            # ÂèçÂêë‰º†Êí≠\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # ÁªüËÆ°ÊåáÊ†á\n",
    "            epoch_loss += loss.item() * x_batch.size(0)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == label_batch).sum().item()\n",
    "            total += label_batch.size(0)\n",
    "\n",
    "            # Êõ¥Êñ∞ËøõÂ∫¶Êù°\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{correct / total:.2%}\"\n",
    "            })\n",
    "\n",
    "    # ËÆ°ÁÆóepochÊåáÊ†á\n",
    "    avg_loss = epoch_loss / len(dataset)\n",
    "    epoch_acc = correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    acc_history.append(epoch_acc)\n",
    "\n",
    "    # Êõ¥Êñ∞Â≠¶‰π†Áéá\n",
    "    scheduler.step()\n",
    "\n",
    "    # ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã\n",
    "    if epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        torch.save(classifier.state_dict(), 'best_noisy_classifier.pth')\n",
    "        print(f\"‚úÖ ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã | ÂáÜÁ°ÆÁéá: {best_acc:.2%}\")\n",
    "\n",
    "    # ÂÆöÊúü‰øùÂ≠òÊ£ÄÊü•ÁÇπ\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': classifier.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'accuracy': epoch_acc\n",
    "        }, f'classifier_checkpoint_epoch_{epoch + 1}.pth')\n",
    "\n",
    "    # ÊâìÂç∞epochÁªìÊûú\n",
    "    print(f\"Epoch {epoch + 1:03d}/{num_epochs} | \"\n",
    "          f\"Loss: {avg_loss:.4f} | \"\n",
    "          f\"Acc: {epoch_acc:.2%} | \"\n",
    "          f\"LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(acc_history, label='Accuracy', color='orange')\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('classifier_training_curves.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ ËÆ≠ÁªÉÂÆåÊàê!\")\n"
   ],
   "id": "7b98b337c0b88883",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [04:32<00:00,  7.38s/batch, loss=1.8201, acc=18.71%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã | ÂáÜÁ°ÆÁéá: 18.71%\n",
      "Epoch 001/1000 | Loss: 2.0185 | Acc: 18.71% | LR: 3.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  24%|‚ñà‚ñà‚ñç       | 9/37 [01:16<03:57,  8.48s/batch, loss=1.9588, acc=21.70%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     21\u001B[39m noisy_batch = diffusion.q_sample(x_start=x_batch, t=t)\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# ÂàÜÁ±ªÂô®ÂâçÂêë‰º†Êí≠\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m logits = \u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnoisy_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# ÂÅáËÆæÂàÜÁ±ªÂô®Êé•ÂèóÊó∂Èó¥Ê≠•ËæìÂÖ•\u001B[39;00m\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# ËÆ°ÁÆóÊçüÂ§±\u001B[39;00m\n\u001B[32m     27\u001B[39m loss = F.cross_entropy(logits, label_batch)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Learn\\Improved_Diffuion\\src\\classifier.py:252\u001B[39m, in \u001B[36mUNetClassifier.forward\u001B[39m\u001B[34m(self, x, t)\u001B[39m\n\u001B[32m    250\u001B[39m \u001B[38;5;66;03m# ÁºñÁ†ÅÂô®\u001B[39;00m\n\u001B[32m    251\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.down:\n\u001B[32m--> \u001B[39m\u001B[32m252\u001B[39m     x = \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_emb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    253\u001B[39m     skips.append(x)\n\u001B[32m    254\u001B[39m     x = F.max_pool2d(x, kernel_size=(\u001B[32m2\u001B[39m,\u001B[32m1\u001B[39m))  \u001B[38;5;66;03m# ‰øùÊåÅÂÆΩÂ∫¶‰∏çÂèò\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Learn\\Improved_Diffuion\\src\\classifier.py:13\u001B[39m, in \u001B[36mMySequential.forward\u001B[39m\u001B[34m(self, x, t_emb)\u001B[39m\n\u001B[32m     11\u001B[39m         x = module(x, t_emb)\n\u001B[32m     12\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# ÂÖ∂‰ªñÊ®°ÂùóÊåâÈªòËÆ§ÊñπÂºèÂ§ÑÁêÜ\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m         x = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Learn\\Improved_Diffuion\\src\\classifier.py:136\u001B[39m, in \u001B[36mResidual.forward\u001B[39m\u001B[34m(self, x, *args, **kwargs)\u001B[39m\n\u001B[32m    135\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m136\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m + x\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Learn\\Improved_Diffuion\\src\\classifier.py:201\u001B[39m, in \u001B[36mPreNorm.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    199\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m    200\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm(x)\n\u001B[32m--> \u001B[39m\u001B[32m201\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Learn\\Improved_Diffuion\\src\\classifier.py:178\u001B[39m, in \u001B[36mLinearAttention.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m    177\u001B[39m     b, c, h, w = x.shape\n\u001B[32m--> \u001B[39m\u001B[32m178\u001B[39m     qkv = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mto_qkv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m.chunk(\u001B[32m3\u001B[39m, dim=\u001B[32m1\u001B[39m)\n\u001B[32m    179\u001B[39m     q, k, v = \u001B[38;5;28mmap\u001B[39m(\n\u001B[32m    180\u001B[39m         \u001B[38;5;28;01mlambda\u001B[39;00m t: rearrange(t, \u001B[33m\"\u001B[39m\u001B[33mb (h c) x y -> b h c (x y)\u001B[39m\u001B[33m\"\u001B[39m, h=\u001B[38;5;28mself\u001B[39m.heads), qkv\n\u001B[32m    181\u001B[39m     )\n\u001B[32m    183\u001B[39m     q = q.softmax(dim=-\u001B[32m2\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\miniconda3\\envs\\guided_diffusion\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    537\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    538\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    539\u001B[39m         F.pad(\n\u001B[32m    540\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    547\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    548\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
